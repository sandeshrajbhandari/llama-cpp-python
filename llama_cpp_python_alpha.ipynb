{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "-w5V1w10KLKl",
        "loNlSst2jRSr"
      ],
      "authorship_tag": "ABX9TyMRAWq56b9WLkbwjAkUbTfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeshrajbhandari/llama-cpp-python/blob/main/llama_cpp_python_alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-cpp-python\n",
        "!pip uninstall llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python[server]==0.1.64"
      ],
      "metadata": {
        "id": "GwANLcd9ce1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd72e43b-e3b1-4098-b5c0-58cede3924cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-cpp-python==0.1.64\n",
            "  Downloading llama_cpp_python-0.1.64.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (1.22.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.64)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.64-cp310-cp310-linux_x86_64.whl size=355754 sha256=509b4ce8f51b5a5f17e2473353deea81a717763060c74eda3ff5010c847a02f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/64/4e/74ef44b95c143b120b87d45836a4949ddee2ab37eb5aaa4a83\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python[server]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6DUqwZkgjGU",
        "outputId": "6715fc1c-a02f-424d-ec95-111cf87452f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python[server] in /usr/local/lib/python3.10/dist-packages (0.1.64)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (1.22.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python[server]) (5.6.1)\n",
            "Collecting uvicorn>=0.21.1 (from llama-cpp-python[server])\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.0 (from llama-cpp-python[server])\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette>=1.3.3 (from llama-cpp-python[server])\n",
            "  Downloading sse_starlette-1.6.1-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.0->llama-cpp-python[server]) (1.10.9)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.0->llama-cpp-python[server])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.21.1->llama-cpp-python[server]) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.21.1->llama-cpp-python[server])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python[server]) (3.7.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python[server]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python[server]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python[server]) (1.1.1)\n",
            "Installing collected packages: h11, uvicorn, starlette, sse-starlette, fastapi\n",
            "Successfully installed fastapi-0.100.0 h11-0.14.0 sse-starlette-1.6.1 starlette-0.27.0 uvicorn-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir\n",
        "!mkdir llama.cpp/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrOjrwaxIzzz",
        "outputId": "6b5108d9-1640-4a56-fee1-4251253d9cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: missing operand\n",
            "Try 'mkdir --help' for more information.\n",
            "mkdir: cannot create directory ‘llama.cpp/models’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Ac0p1EGrXQ",
        "outputId": "72151f2c-3a6d-44c0-befd-8e32dd141141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cido9CEFtnFL",
        "outputId": "ceff547c-7e3d-46bb-c662-ca16aafacbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-cpp-python  models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=\"/content/llama.cpp/models/llama-7b.ggmlv3.q4_0.bin\", n_gpu_layers=25)\n",
        "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n",
        "print(output.choices[0][\"text\"])"
      ],
      "metadata": {
        "id": "3F7Ss395FxPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGgtS7lzgRvV",
        "outputId": "f9a7e481-98c1-40ae-b44d-69510e5b42cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Name the planets in the solar system? A: 1. Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto B) Venus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Llama??"
      ],
      "metadata": {
        "id": "OgM93k9kf0-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwkIyU9tIw0A",
        "outputId": "87141f52-0e91-42ea-85e0-a75926e7e5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-6fd19a0c-fd7b-4f6e-8cf7-3321ee7e6ea8', 'object': 'text_completion', 'created': 1687344430, 'model': '/content/llama.cpp/models/llama-7b.ggmlv3.q4_0.bin', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1. Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto B) Venus', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 32, 'total_tokens': 47}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlwhjypEIpk3",
        "outputId": "a488c12b-fae8-4f4e-b9c1-17dddcd8ab84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Name the planets in the solar system? A: 1. Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto B) Venus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-cpp-python[server]"
      ],
      "metadata": {
        "id": "hBKmYr6kaKZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose model"
      ],
      "metadata": {
        "id": "KtlOcXKwity0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # !wget https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin /content/llama.cpp/models/\n",
        "\n",
        "model_link = 'https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/blob/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin' #@param [\"https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GGML/resolve/main/Wizard-Vicuna-30B-Uncensored.ggmlv3.q4_0.bin\", \"https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML/resolve/main/nous-hermes-13b.ggmlv3.q4_0.bin\",\"https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin\", \"https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/blob/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin\"] {allow-input: true}\n",
        "# !wget $model_link -P /content/llama.cpp/models/"
      ],
      "metadata": {
        "id": "V20GEbHOGLUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fast download hack\n",
        "using hf_transfer, src: https://twitter.com/TheBlokeAI/status/1677360987333173266"
      ],
      "metadata": {
        "id": "-w5V1w10KLKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub, hf_transfer\n",
        "import os\n",
        "\n",
        "# os.environ[\"HF_ENDPOINT\"] = \"http://localhost:5564\"\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "fCOrfwNNsoqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930b3938-6053-4a40-e790-791d40d941ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.6.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "# https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin\n",
        "from urllib.parse import urlparse\n",
        "url = model_link\n",
        "parsed_url = urlparse(url)\n",
        "path_segments = parsed_url.path.strip(\"/\").split(\"/\")\n",
        "repo_id = \"/\".join(path_segments[:2])\n",
        "filename = path_segments[-1]\n",
        "\n",
        "# print(\"repo_id:\", repo_id)\n",
        "# print(\"filename:\", filename)\n",
        "\n",
        "hf_hub_download(repo_id=repo_id, filename=filename, local_dir=\"/content/llama.cpp/models\")\n",
        "## downloaded in 2m 6seconds for 30B model\n",
        "## 1m8s for 13B model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "f0SAGeOQtNEY",
        "outputId": "6dc2a6fb-c829-4e08-e790-9463d5d02906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Request 916cd8b7-c5fc-4e93-a483-fdb6fad7fafd: HEAD https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/resolve/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin (authenticated: False)\n",
            "DEBUG:huggingface_hub.utils._http:Request 916cd8b7-c5fc-4e93-a483-fdb6fad7fafd: HEAD https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/resolve/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin (authenticated: False)\n",
            "downloading https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/resolve/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin to /root/.cache/huggingface/hub/tmpwn7lr0yj\n",
            "INFO:huggingface_hub.file_download:downloading https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/resolve/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin to /root/.cache/huggingface/hub/tmpwn7lr0yj\n",
            "Download https://cdn-lfs.huggingface.co/repos/db/27/db27a0024de4a41d1f9c6b1f54b8f2c9d5b957aae84ce389023e029cccaa4d8a/72a995aee66105f7fc60eea7352c19e6c63abdf02ab8ed93ff4901f7eeef52da?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin%3B+filename%3D%22wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1689230921&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTIzMDkyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9kYi8yNy9kYjI3YTAwMjRkZTRhNDFkMWY5YzZiMWY1NGI4ZjJjOWQ1Yjk1N2FhZTg0Y2UzODkwMjNlMDI5Y2NjYWE0ZDhhLzcyYTk5NWFlZTY2MTA1ZjdmYzYwZWVhNzM1MmMxOWU2YzYzYWJkZjAyYWI4ZWQ5M2ZmNDkwMWY3ZWVlZjUyZGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=0CKWphfuCePoytXpNqhTyuZFOWrWEWW0yPuwtr45k56S874IGmm2d7s-Ai11%7E5w-zGPmvcdZC5y%7EXxGzrdIMMv7uhZYNfDc06dHLm1XwqZpKj3WbTDRFaI0SzItKKwNh3cIpL0TdV1u4KuaH7HTYxaJAdNFY1f1McAz5i4yhaHENWJMYskyB0bp-stkzWZCjWOSdcvvrh0VrL0OhPJ4CAvGpf99axH3-dqw%7EwLiOcVvXLYBYnGb0JB3LGgctxWdIMgpbP3B%7En73WTIebUeD5RiK%7EWZbGcS8czSQ-2PkM4on9Mt-yKbUy9G%7EwoE1LSyuLCF8Eo7byLzta09cxsL0hsQ__&Key-Pair-Id=KVTP0A1DKRTAX using HF_TRANSFER.\n",
            "DEBUG:huggingface_hub.file_download:Download https://cdn-lfs.huggingface.co/repos/db/27/db27a0024de4a41d1f9c6b1f54b8f2c9d5b957aae84ce389023e029cccaa4d8a/72a995aee66105f7fc60eea7352c19e6c63abdf02ab8ed93ff4901f7eeef52da?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin%3B+filename%3D%22wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1689230921&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTIzMDkyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9kYi8yNy9kYjI3YTAwMjRkZTRhNDFkMWY5YzZiMWY1NGI4ZjJjOWQ1Yjk1N2FhZTg0Y2UzODkwMjNlMDI5Y2NjYWE0ZDhhLzcyYTk5NWFlZTY2MTA1ZjdmYzYwZWVhNzM1MmMxOWU2YzYzYWJkZjAyYWI4ZWQ5M2ZmNDkwMWY3ZWVlZjUyZGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=0CKWphfuCePoytXpNqhTyuZFOWrWEWW0yPuwtr45k56S874IGmm2d7s-Ai11%7E5w-zGPmvcdZC5y%7EXxGzrdIMMv7uhZYNfDc06dHLm1XwqZpKj3WbTDRFaI0SzItKKwNh3cIpL0TdV1u4KuaH7HTYxaJAdNFY1f1McAz5i4yhaHENWJMYskyB0bp-stkzWZCjWOSdcvvrh0VrL0OhPJ4CAvGpf99axH3-dqw%7EwLiOcVvXLYBYnGb0JB3LGgctxWdIMgpbP3B%7En73WTIebUeD5RiK%7EWZbGcS8czSQ-2PkM4on9Mt-yKbUy9G%7EwoE1LSyuLCF8Eo7byLzta09cxsL0hsQ__&Key-Pair-Id=KVTP0A1DKRTAX using HF_TRANSFER.\n",
            "Storing https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/resolve/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin in cache at /root/.cache/huggingface/hub/models--TheBloke--WizardLM-13B-V1.0-Uncensored-GGML/blobs/72a995aee66105f7fc60eea7352c19e6c63abdf02ab8ed93ff4901f7eeef52da\n",
            "INFO:huggingface_hub.file_download:Storing https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML/resolve/main/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin in cache at /root/.cache/huggingface/hub/models--TheBloke--WizardLM-13B-V1.0-Uncensored-GGML/blobs/72a995aee66105f7fc60eea7352c19e6c63abdf02ab8ed93ff4901f7eeef52da\n",
            "Create symlink to local dir\n",
            "INFO:huggingface_hub.file_download:Create symlink to local dir\n",
            "Creating pointer from ../../../root/.cache/huggingface/hub/models--TheBloke--WizardLM-13B-V1.0-Uncensored-GGML/blobs/72a995aee66105f7fc60eea7352c19e6c63abdf02ab8ed93ff4901f7eeef52da to /content/llama.cpp/models/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin\n",
            "INFO:huggingface_hub.file_download:Creating pointer from ../../../root/.cache/huggingface/hub/models--TheBloke--WizardLM-13B-V1.0-Uncensored-GGML/blobs/72a995aee66105f7fc60eea7352c19e6c63abdf02ab8ed93ff4901f7eeef52da to /content/llama.cpp/models/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/llama.cpp/models/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pqyGtaVVkLMm",
        "outputId": "fbb7e405-b84c-432b-bc38-753212baec50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # !wget https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin /content/llama.cpp/models/\n",
        "\n",
        "# model_link = 'https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin' #@param [\"https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GGML/resolve/main/Wizard-Vicuna-30B-Uncensored.ggmlv3.q4_0.bin\", \"https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML/resolve/main/nous-hermes-13b.ggmlv3.q4_0.bin\",\"https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin\"] {allow-input: true}\n",
        "# !wget $model_link -P /content/llama.cpp/models/"
      ],
      "metadata": {
        "id": "jftng9ThTzBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd llama.cpp"
      ],
      "metadata": {
        "id": "qmlFcd8WTzBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## custom llama-cpp-python installation"
      ],
      "metadata": {
        "id": "loNlSst2jRSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(\"/content/llama-cpp-python\")"
      ],
      "metadata": {
        "id": "OLDyX5Lxtnbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/sandeshrajbhandari/llama-cpp-python.git\n",
        "!git clone --recurse-submodules https://github.com/sandeshrajbhandari/llama-cpp-python.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN2ywl6yRa8b",
        "outputId": "a2c1e8d9-94b8-4caa-bcdf-f07bf7fa4e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama-cpp-python'...\n",
            "remote: Enumerating objects: 2821, done.\u001b[K\n",
            "remote: Counting objects: 100% (1263/1263), done.\u001b[K\n",
            "remote: Compressing objects: 100% (254/254), done.\u001b[K\n",
            "remote: Total 2821 (delta 1118), reused 1029 (delta 1006), pack-reused 1558\u001b[K\n",
            "Receiving objects: 100% (2821/2821), 591.22 KiB | 16.89 MiB/s, done.\n",
            "Resolving deltas: 100% (1757/1757), done.\n",
            "Submodule 'vendor/llama.cpp' (https://github.com/ggerganov/llama.cpp.git) registered for path 'vendor/llama.cpp'\n",
            "Cloning into '/content/llama-cpp-python/vendor/llama.cpp'...\n",
            "remote: Enumerating objects: 4687, done.        \n",
            "remote: Counting objects: 100% (2266/2266), done.        \n",
            "remote: Compressing objects: 100% (489/489), done.        \n",
            "remote: Total 4687 (delta 2043), reused 1890 (delta 1777), pack-reused 2421        \n",
            "Receiving objects: 100% (4687/4687), 3.93 MiB | 5.08 MiB/s, done.\n",
            "Resolving deltas: 100% (3184/3184), done.\n",
            "Submodule path 'vendor/llama.cpp': checked out '8596af427722775f0df4a7c90b9af067ba90d4ef'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTJmeSh1ilmM",
        "outputId": "78558b27-34d0-4327-eed3-c89a62482bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-build\n",
            "  Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "Collecting distro (from scikit-build)\n",
            "  Using cached distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scikit-build) (23.1)\n",
            "Requirement already satisfied: setuptools>=42.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build) (67.7.2)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from scikit-build) (2.0.1)\n",
            "Requirement already satisfied: wheel>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build) (0.40.0)\n",
            "Installing collected packages: distro, scikit-build\n",
            "Successfully installed distro-1.8.0 scikit-build-0.17.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 python3 setup.py develop\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install -e .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "-z9TuVpLU388"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama-cpp-python\n",
        "# !pip install scikit-build\n",
        "# install llama.cpp\n",
        "# !python3 setup.py develop\n",
        "# !pip install -e .\n",
        "# install llama.cpp server\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install -e .[server]\n",
        "%cd .."
      ],
      "metadata": {
        "id": "koO0HUpgqI9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11396ad-4eb9-4958-9e03-adc6824ce46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama-cpp-python\n",
            "Obtaining file:///content/llama-cpp-python\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (1.22.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (5.6.1)\n",
            "Requirement already satisfied: uvicorn>=0.21.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (0.22.0)\n",
            "Requirement already satisfied: fastapi>=0.95.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (0.100.0)\n",
            "Requirement already satisfied: sse-starlette>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (1.6.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.64) (1.5.6)\n",
            "Collecting pyngrok>=5.1.0 (from llama-cpp-python==0.1.64)\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.0->llama-cpp-python==0.1.64) (1.10.9)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.0->llama-cpp-python==0.1.64) (0.27.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok>=5.1.0->llama-cpp-python==0.1.64) (6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.21.1->llama-cpp-python==0.1.64) (8.1.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.21.1->llama-cpp-python==0.1.64) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python==0.1.64) (3.7.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python==0.1.64) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python==0.1.64) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.0->llama-cpp-python==0.1.64) (1.1.1)\n",
            "Building wheels for collected packages: pyngrok, llama-cpp-python\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=d375e9dc18a6fa36c6155e4205e6cde9b5d395dc70b14eaa0ab575a2488a762c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "  Building editable for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.64-0.editable-py3-none-any.whl size=6303 sha256=45c9042a140ab82ca46275f258270d36e0afd5286155b0756ef178ab3a95c037\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rcnoksax/wheels/ff/4c/c1/ce38165dd7770455323458ae3e325455e30604af7a1a283e60\n",
            "Successfully built pyngrok llama-cpp-python\n",
            "Installing collected packages: pyngrok, llama-cpp-python\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama-cpp-python 0.1.64\n",
            "    Uninstalling llama-cpp-python-0.1.64:\n",
            "      Successfully uninstalled llama-cpp-python-0.1.64\n",
            "Successfully installed llama-cpp-python-0.1.64 pyngrok-6.0.0\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyngrok\n",
        "# !pip install nest-asyncio"
      ],
      "metadata": {
        "id": "hW96cgyifWGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2OiTRsdlE8EnoupBcmMyp7kxI6u_7wsEFM22wLSdr5aztEhBW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8EalaatxfKE",
        "outputId": "1275fb01-851d-4ea1-a432-e94d554ee352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN SERVER"
      ],
      "metadata": {
        "id": "nbZKVsSkUE2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m llama_cpp.server --model /content/llama.cpp/models/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin --host 127.0.0.1 --port 2600 --use_ngrok True --n_gpu_layers 2000\n",
        "# !python3 -m llama_cpp.server --model /content/llama.cpp/models/wizardlm-13b-v1.0-superhot-8k.ggmlv3.q4_K_M.bin --host 127.0.0.1 --port 2600 --use_ngrok True --n_gpu_layers 2000"
      ],
      "metadata": {
        "id": "3xYwgnm4G3E_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e290caa-d60d-4f58-db41-0e3c4b62260f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4\n",
            "changing host and port to 127.0.0.1 and 2600 for using with ngrok in colab\n",
            "llama.cpp: loading model from /content/llama.cpp/models/wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 2048\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "warning: failed to mlock 94208-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
            "Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\n",
            "llama_model_load_internal: using CUDA for GPU acceleration\n",
            "llama_model_load_internal: mem required  = 2135.98 MB (+ 1608.00 MB per state)\n",
            "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
            "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
            "llama_model_load_internal: offloading non-repeating layers to GPU\n",
            "llama_model_load_internal: offloading v cache to GPU\n",
            "llama_model_load_internal: offloading k cache to GPU\n",
            "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
            "llama_model_load_internal: total VRAM used: 9016 MB\n",
            "warning: failed to mlock 92160000-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
            "Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\n",
            "....................................................................................................\n",
            "llama_init_from_file: kv self size  = 1600.00 MB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n",
            "Password: \n",
            "t=2023-07-10T06:53:09+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "Public URL: https://e674-34-124-211-255.ngrok-free.app\n",
            "Go to https://e674-34-124-211-255.ngrok-free.app/docs to see the interactive API docs.\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8467\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:2600\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     1.57 ms /     3 runs   (    0.52 ms per token)\n",
            "llama_print_timings: prompt eval time =   865.59 ms /    25 tokens (   34.62 ms per token)\n",
            "llama_print_timings:        eval time =   215.15 ms /     2 runs   (  107.57 ms per token)\n",
            "llama_print_timings:       total time =  1154.23 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     8.22 ms /     8 runs   (    1.03 ms per token)\n",
            "llama_print_timings: prompt eval time =   831.38 ms /    26 tokens (   31.98 ms per token)\n",
            "llama_print_timings:        eval time =   892.31 ms /     7 runs   (  127.47 ms per token)\n",
            "llama_print_timings:       total time =  1915.38 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mOPTIONS /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     5.42 ms /    10 runs   (    0.54 ms per token)\n",
            "llama_print_timings: prompt eval time =   799.08 ms /     8 tokens (   99.89 ms per token)\n",
            "llama_print_timings:        eval time =  1213.27 ms /     9 runs   (  134.81 ms per token)\n",
            "llama_print_timings:       total time =  2077.87 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.03 ms /    16 runs   (    0.56 ms per token)\n",
            "llama_print_timings: prompt eval time =   825.80 ms /    25 tokens (   33.03 ms per token)\n",
            "llama_print_timings:        eval time =  1937.82 ms /    15 runs   (  129.19 ms per token)\n",
            "llama_print_timings:       total time =  2933.11 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    10.62 ms /    16 runs   (    0.66 ms per token)\n",
            "llama_print_timings: prompt eval time =  1487.11 ms /    70 tokens (   21.24 ms per token)\n",
            "llama_print_timings:        eval time =  1825.72 ms /    15 runs   (  121.71 ms per token)\n",
            "llama_print_timings:       total time =  3769.57 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    16.79 ms /    16 runs   (    1.05 ms per token)\n",
            "llama_print_timings: prompt eval time =  1136.92 ms /    55 tokens (   20.67 ms per token)\n",
            "llama_print_timings:        eval time =  1864.82 ms /    15 runs   (  124.32 ms per token)\n",
            "llama_print_timings:       total time =  3420.89 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     8.85 ms /    16 runs   (    0.55 ms per token)\n",
            "llama_print_timings: prompt eval time =   812.62 ms /    18 tokens (   45.15 ms per token)\n",
            "llama_print_timings:        eval time =  1761.44 ms /    15 runs   (  117.43 ms per token)\n",
            "llama_print_timings:       total time =  2925.77 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    15.95 ms /    16 runs   (    1.00 ms per token)\n",
            "llama_print_timings: prompt eval time =   805.67 ms /    19 tokens (   42.40 ms per token)\n",
            "llama_print_timings:        eval time =  1979.10 ms /    15 runs   (  131.94 ms per token)\n",
            "llama_print_timings:       total time =  2954.98 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.03 ms /    16 runs   (    0.56 ms per token)\n",
            "llama_print_timings: prompt eval time =   800.02 ms /    25 tokens (   32.00 ms per token)\n",
            "llama_print_timings:        eval time =  2022.71 ms /    15 runs   (  134.85 ms per token)\n",
            "llama_print_timings:       total time =  2950.77 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mOPTIONS /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.17 ms /    16 runs   (    0.57 ms per token)\n",
            "llama_print_timings: prompt eval time =   809.57 ms /    29 tokens (   27.92 ms per token)\n",
            "llama_print_timings:        eval time =  2033.32 ms /    15 runs   (  135.55 ms per token)\n",
            "llama_print_timings:       total time =  2983.20 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     4.95 ms /     9 runs   (    0.55 ms per token)\n",
            "llama_print_timings: prompt eval time =  1521.22 ms /    73 tokens (   20.84 ms per token)\n",
            "llama_print_timings:        eval time =  1092.15 ms /     8 runs   (  136.52 ms per token)\n",
            "llama_print_timings:       total time =  2834.83 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     8.64 ms /    16 runs   (    0.54 ms per token)\n",
            "llama_print_timings: prompt eval time =  1122.81 ms /    61 tokens (   18.41 ms per token)\n",
            "llama_print_timings:        eval time =  1889.03 ms /    15 runs   (  125.94 ms per token)\n",
            "llama_print_timings:       total time =  3285.41 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    11.35 ms /    16 runs   (    0.71 ms per token)\n",
            "llama_print_timings: prompt eval time =   815.74 ms /    17 tokens (   47.98 ms per token)\n",
            "llama_print_timings:        eval time =  1873.38 ms /    15 runs   (  124.89 ms per token)\n",
            "llama_print_timings:       total time =  2930.21 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    15.65 ms /    16 runs   (    0.98 ms per token)\n",
            "llama_print_timings: prompt eval time =   800.83 ms /    25 tokens (   32.03 ms per token)\n",
            "llama_print_timings:        eval time =  1945.15 ms /    15 runs   (  129.68 ms per token)\n",
            "llama_print_timings:       total time =  2942.26 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    10.31 ms /    16 runs   (    0.64 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =  2131.74 ms /    16 runs   (  133.23 ms per token)\n",
            "llama_print_timings:       total time =  2212.10 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.03 ms /    16 runs   (    0.56 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =  2175.61 ms /    16 runs   (  135.98 ms per token)\n",
            "llama_print_timings:       total time =  2247.72 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    11.92 ms /    16 runs   (    0.75 ms per token)\n",
            "llama_print_timings: prompt eval time =   798.98 ms /    27 tokens (   29.59 ms per token)\n",
            "llama_print_timings:        eval time =  1993.74 ms /    15 runs   (  132.92 ms per token)\n",
            "llama_print_timings:       total time =  2945.76 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    21.46 ms /    16 runs   (    1.34 ms per token)\n",
            "llama_print_timings: prompt eval time =   818.10 ms /    25 tokens (   32.72 ms per token)\n",
            "llama_print_timings:        eval time =  1920.31 ms /    15 runs   (  128.02 ms per token)\n",
            "llama_print_timings:       total time =  2946.10 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.13 ms /    16 runs   (    0.57 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =  2115.09 ms /    16 runs   (  132.19 ms per token)\n",
            "llama_print_timings:       total time =  2193.31 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     4.94 ms /     8 runs   (    0.62 ms per token)\n",
            "llama_print_timings: prompt eval time =   795.71 ms /    14 tokens (   56.84 ms per token)\n",
            "llama_print_timings:        eval time =   933.32 ms /     7 runs   (  133.33 ms per token)\n",
            "llama_print_timings:       total time =  1799.08 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     8.42 ms /     8 runs   (    1.05 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =  1034.47 ms /     8 runs   (  129.31 ms per token)\n",
            "llama_print_timings:       total time =  1089.59 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     1.61 ms /     2 runs   (    0.80 ms per token)\n",
            "llama_print_timings: prompt eval time =   822.33 ms /    26 tokens (   31.63 ms per token)\n",
            "llama_print_timings:        eval time =    93.74 ms /     1 runs   (   93.74 ms per token)\n",
            "llama_print_timings:       total time =   989.72 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     3.11 ms /     5 runs   (    0.62 ms per token)\n",
            "llama_print_timings: prompt eval time =   807.25 ms /     7 tokens (  115.32 ms per token)\n",
            "llama_print_timings:        eval time =   541.59 ms /     4 runs   (  135.40 ms per token)\n",
            "llama_print_timings:       total time =  1398.03 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =   142.98 ms /   215 runs   (    0.67 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time = 26485.90 ms /   215 runs   (  123.19 ms per token)\n",
            "llama_print_timings:       total time = 33085.85 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     6.91 ms /    11 runs   (    0.63 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =  1252.84 ms /    11 runs   (  113.89 ms per token)\n",
            "llama_print_timings:       total time =  1464.80 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    10.95 ms /    19 runs   (    0.58 ms per token)\n",
            "llama_print_timings: prompt eval time =  1124.08 ms /    59 tokens (   19.05 ms per token)\n",
            "llama_print_timings:        eval time =  2211.02 ms /    18 runs   (  122.83 ms per token)\n",
            "llama_print_timings:       total time =  3631.89 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     1.07 ms /     2 runs   (    0.54 ms per token)\n",
            "llama_print_timings: prompt eval time =   669.70 ms /     5 tokens (  133.94 ms per token)\n",
            "llama_print_timings:        eval time =    82.17 ms /     1 runs   (   82.17 ms per token)\n",
            "llama_print_timings:       total time =   771.17 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     5.81 ms /    10 runs   (    0.58 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =  1243.67 ms /    10 runs   (  124.37 ms per token)\n",
            "llama_print_timings:       total time =  1317.10 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    24.18 ms /    10 runs   (    2.42 ms per token)\n",
            "llama_print_timings: prompt eval time =   855.40 ms /    10 tokens (   85.54 ms per token)\n",
            "llama_print_timings:        eval time =  1132.14 ms /     9 runs   (  125.79 ms per token)\n",
            "llama_print_timings:       total time =  2187.16 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     5.89 ms /    10 runs   (    0.59 ms per token)\n",
            "llama_print_timings: prompt eval time =   733.15 ms /     5 tokens (  146.63 ms per token)\n",
            "llama_print_timings:        eval time =  1217.89 ms /     9 runs   (  135.32 ms per token)\n",
            "llama_print_timings:       total time =  1999.94 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.22 ms /    16 runs   (    0.58 ms per token)\n",
            "llama_print_timings: prompt eval time =   786.80 ms /    18 tokens (   43.71 ms per token)\n",
            "llama_print_timings:        eval time =  2032.56 ms /    15 runs   (  135.50 ms per token)\n",
            "llama_print_timings:       total time =  2924.60 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.52 ms /    16 runs   (    0.60 ms per token)\n",
            "llama_print_timings: prompt eval time =   799.41 ms /    12 tokens (   66.62 ms per token)\n",
            "llama_print_timings:        eval time =  1939.95 ms /    15 runs   (  129.33 ms per token)\n",
            "llama_print_timings:       total time =  2857.46 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    14.62 ms /    16 runs   (    0.91 ms per token)\n",
            "llama_print_timings: prompt eval time =   845.90 ms /    10 tokens (   84.59 ms per token)\n",
            "llama_print_timings:        eval time =  1926.99 ms /    15 runs   (  128.47 ms per token)\n",
            "llama_print_timings:       total time =  2925.82 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     9.03 ms /    16 runs   (    0.56 ms per token)\n",
            "llama_print_timings: prompt eval time =   856.17 ms /    10 tokens (   85.62 ms per token)\n",
            "llama_print_timings:        eval time =  1928.54 ms /    15 runs   (  128.57 ms per token)\n",
            "llama_print_timings:       total time =  2915.71 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    13.65 ms /    16 runs   (    0.85 ms per token)\n",
            "llama_print_timings: prompt eval time =   942.75 ms /    23 tokens (   40.99 ms per token)\n",
            "llama_print_timings:        eval time =  1934.87 ms /    15 runs   (  128.99 ms per token)\n",
            "llama_print_timings:       total time =  3068.68 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    16.20 ms /    16 runs   (    1.01 ms per token)\n",
            "llama_print_timings: prompt eval time =   816.30 ms /    25 tokens (   32.65 ms per token)\n",
            "llama_print_timings:        eval time =  1969.19 ms /    15 runs   (  131.28 ms per token)\n",
            "llama_print_timings:       total time =  2997.09 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =   282.88 ms /   408 runs   (    0.69 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time = 47003.59 ms /   408 runs   (  115.20 ms per token)\n",
            "llama_print_timings:       total time = 85913.48 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     1.09 ms /     2 runs   (    0.55 ms per token)\n",
            "llama_print_timings: prompt eval time =   669.77 ms /     5 tokens (  133.95 ms per token)\n",
            "llama_print_timings:        eval time =    99.15 ms /     1 runs   (   99.15 ms per token)\n",
            "llama_print_timings:       total time =   992.63 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     4.63 ms /     8 runs   (    0.58 ms per token)\n",
            "llama_print_timings: prompt eval time =   818.53 ms /    26 tokens (   31.48 ms per token)\n",
            "llama_print_timings:        eval time =   856.83 ms /     7 runs   (  122.40 ms per token)\n",
            "llama_print_timings:       total time =  1770.69 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     1.27 ms /     1 runs   (    1.27 ms per token)\n",
            "llama_print_timings: prompt eval time =   813.18 ms /     8 tokens (  101.65 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)\n",
            "llama_print_timings:       total time =   868.80 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     0.52 ms /     1 runs   (    0.52 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =   124.94 ms /     1 runs   (  124.94 ms per token)\n",
            "llama_print_timings:       total time =   128.68 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     0.99 ms /     1 runs   (    0.99 ms per token)\n",
            "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
            "llama_print_timings:        eval time =   127.11 ms /     1 runs   (  127.11 ms per token)\n",
            "llama_print_timings:       total time =   133.51 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =   169.39 ms /   200 runs   (    0.85 ms per token)\n",
            "llama_print_timings: prompt eval time =   779.74 ms /    25 tokens (   31.19 ms per token)\n",
            "llama_print_timings:        eval time = 24788.35 ms /   199 runs   (  124.56 ms per token)\n",
            "llama_print_timings:       total time = 43191.51 ms\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     4.70 ms /     8 runs   (    0.59 ms per token)\n",
            "llama_print_timings: prompt eval time =   755.71 ms /    26 tokens (   29.07 ms per token)\n",
            "llama_print_timings:        eval time =   896.22 ms /     7 runs   (  128.03 ms per token)\n",
            "llama_print_timings:       total time =  1826.00 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    43.42 ms /    50 runs   (    0.87 ms per token)\n",
            "llama_print_timings: prompt eval time =   786.96 ms /    25 tokens (   31.48 ms per token)\n",
            "llama_print_timings:        eval time =  6005.77 ms /    49 runs   (  122.57 ms per token)\n",
            "llama_print_timings:       total time =  7182.56 ms\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /v1/completions?max_tokens=512 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =     6.49 ms /     8 runs   (    0.81 ms per token)\n",
            "llama_print_timings: prompt eval time =   828.05 ms /    26 tokens (   31.85 ms per token)\n",
            "llama_print_timings:        eval time =   860.89 ms /     7 runs   (  122.98 ms per token)\n",
            "llama_print_timings:       total time =  1865.92 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =   161.48 ms /   217 runs   (    0.74 ms per token)\n",
            "llama_print_timings: prompt eval time =   800.52 ms /    20 tokens (   40.03 ms per token)\n",
            "llama_print_timings:        eval time = 28662.63 ms /   216 runs   (  132.70 ms per token)\n",
            "llama_print_timings:       total time = 31540.76 ms\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    10.17 ms /    10 runs   (    1.02 ms per token)\n",
            "llama_print_timings: prompt eval time =   806.62 ms /     8 tokens (  100.83 ms per token)\n",
            "llama_print_timings:        eval time =  1124.73 ms /     9 runs   (  124.97 ms per token)\n",
            "llama_print_timings:       total time =  2117.26 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   865.75 ms\n",
            "llama_print_timings:      sample time =    41.73 ms /    55 runs   (    0.76 ms per token)\n",
            "llama_print_timings: prompt eval time =   773.18 ms /    14 tokens (   55.23 ms per token)\n",
            "llama_print_timings:        eval time =  7075.82 ms /    54 runs   (  131.03 ms per token)\n",
            "llama_print_timings:       total time =  8191.90 ms\n",
            "\u001b[32mINFO\u001b[0m:     34.27.79.188:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\u001b[32mINFO\u001b[0m:     27.34.58.64:0 - \"\u001b[1mGET /v1/v1/models HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !export MODEL=/content/llama.cpp/models/llama-7b.ggmlv3.q4_0.bin HOST=0.0.0.0 PORT=2600\n",
        "!python3 -m llama_cpp.server"
      ],
      "metadata": {
        "id": "8JkBHBycbqxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrok Setup"
      ],
      "metadata": {
        "id": "9JUeZr5LJzWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /drive/ngrok-ssh\n",
        "%cd /drive/ngrok-ssh\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n",
        "!unzip -u ngrok-stable-linux-amd64.zip\n",
        "!cp /drive/ngrok-ssh/ngrok /ngrok\n",
        "!chmod +x /ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RXHfXRCJyD-",
        "outputId": "7cf9db51-6622-493d-906c-2162f752fa98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/drive/ngrok-ssh\n",
            "--2023-06-19 14:55:16--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  5.52MB/s    in 2.4s    \n",
            "\n",
            "2023-06-19 14:55:19 (5.52 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/ngrok authtoken 2OiTRsdlE8EnoupBcmMyp7kxI6u_7wsEFM22wLSdr5aztEhBW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF8n10q7J5VO",
        "outputId": "db3d5ed4-af00-4ebd-e238-a62f3b8992fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/http8000.yml\n",
        "tunnels:\n",
        "  http8000:\n",
        "    proto: http\n",
        "    addr: 8000\n",
        "    inspect: false\n",
        "    bind_tls: true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1wg-IvaKPsW",
        "outputId": "594f9619-4cf4-40a2-95f2-87dc554dfaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/http8000.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/ngrok start http8000 --config /root/.ngrok2/ngrok.yml --config /content/http8000.yml \"$@\""
      ],
      "metadata": {
        "id": "NEtWnYXqKuy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ngrok attempt 2"
      ],
      "metadata": {
        "id": "7g7czkWtZnHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNWWzSKjZo5h",
        "outputId": "b8b5c19a-0d03-4c4b-d70f-d1e6abd0d6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=e86cadf1d9caed32926bf2f8e6dda70965f1fe50cf682ab7d282a38fd4ed670b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "# app = Flask(__name__)\n",
        "port = 2600\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "# app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "# ... Update inbound traffic via APIs to use the public-facing ngrok URL\n",
        "\n",
        "\n",
        "# Define Flask routes\n",
        "# @app.route(\"/\")\n",
        "# def index():\n",
        "#     return \"Hello from Colab!\"\n",
        "\n",
        "# Start the Flask server in a new thread\n",
        "# threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()"
      ],
      "metadata": {
        "id": "zQtX1l35ZsEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_cpp\n",
        "import ctypes\n",
        "params = llama_cpp.llama_context_default_params()\n",
        "# use bytes for char * params\n",
        ">>> ctx = llama_cpp.llama_init_from_file(b\"./models/7b/ggml-model.bin\", params)\n",
        ">>> max_tokens = params.n_ctx\n",
        "# use ctypes arrays for array params\n",
        ">>> tokens = (llama_cpp.llama_token * int(max_tokens))()\n",
        ">>> n_tokens = llama_cpp.llama_tokenize(ctx, b\"Q: Name the planets in the solar system? A: \", tokens, max_tokens, add_bos=llama_cpp.c_bool(True))\n",
        ">>> llama_cpp.llama_free(ctx)"
      ],
      "metadata": {
        "id": "22apITVec9g4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}